---
## Scalable Kernel TCP Design and Implementation for Short-Lived Connections
---
### 背景介绍
随着计算机性能的提升，计算机处理能力也在不断地变化，原先能够很好满足在单核上的系统现在随着核数的扩展，也需要不断扩展，理想的情况下，期望得到一个线性的扩展效果，然而，计算机是一个复杂的系统，在多核的架构上，需要共享很多的系统变量，在访问这些共享变量，为了防止发生冲突，需要用锁来对共享资源加以管理，这就造成很很多开销，从而达不到良好的线性开销，因此如何解决共享资源的问题，是一个研究的课题。

随着互联网的发展，网络也越来越成为支持我们生活的一个重要基础设置，如何提升网络的性能，尤其是在多核架构下的网络性能也成为一个研究的课题。一个显著的问题是短连接性能的优化。近年来，移动设备和应用程序快速增长。这些新的移动应用程序产生大量的网络负载构成TCP短连接。HTTP是一个典型的TCP短连接源头。例如，在新浪微博，一个调用大量的HTTP接口的典型要求的长度大约是600个字节，相应的响应的长度通常是1200字节。请求和响应只消耗一个单一的IP数据包，在两组数据交换完成，连接被关闭，这是短连接的特点。在这种情况下，TCP连接的建立和终止的速度成为服务器性能关键。

一个机器CPU内核的数量不断增加，网络协议栈的可扩展性对于多核平台的网络应用程序的性能起着关键的作用。对于长连接新连接的元数据管理不够频繁造，并没有看到在这些情况下TCP协议的可扩展性问题。然而，对于短连接来说，达到同一水平的可扩展性是一个挑战，因为它涉及到频繁和开销很大的TCP连接的建立和终止，从而导致在TCP控制块和虚拟文件系统中严重的资源共享冲突。

作者提出了别fastsocket的可扩展协议栈，尝试通过从表分区级上解决多核下的TCP扩展性问题，此外还有很多相关的工作也基于此展开。

### 相关工作
从本质上来看，造成扩展性问题的根源是多核环境下的很多共享资源的访问，在Linux中，与网络有关的共享资源是iNode和dentry，这被许多工作判断为网络的一个重要瓶颈。有需要的研究围绕这些展开：
>* 如何减少共享资源的访问问题;
>* 如何实现从资源的局部化；
>* 批处理系统调用和零拷贝；
>* 细化共享资源处理；
>* 用户态协议栈

在Linux的设计哲学中，大道至简，一切都可以看成文件，这对于系统编程来说，确实提供了便利，在设计中，这些资源的统一管理就成为一个很重要的问题，Linux的设计者在设计时，采用了基于锁的设计思想，然而对于网络而言，很多共享资源都是没有必要处理的，因此如何剥离这些共享资源，成为扩展问题的关键。

### 挑战
TCB管理：TCP连接的建立和终止操作涉及到管理就是在Linux内核中由TCP套接字代表的全局TCBs。这些所有scokets被管理在两个全局哈希表中，称为listen table，establish table。由于这两个表全系统共享，CPU内核之间的同步是不可避免的。

可扩展性的瓶颈主要是TCP控制块管理和虚拟文件系统的抽象。对于TCP控制块管理，主要是各种锁机制，多个CPU核同时更新该TCBs.当多个客户端同时请求同一个端口建立主动连接，全局侦听socket就用来在所有的cpu内核上面建立连接。

一个新的套接字创建并添加到建立的表，表示一个已建立的连接的主动与被动的关系连接。Linux目前采用每斗锁同步并发修改了多个CPU内核共享表。锁粒度在服务于8核几百个请求时工作还不错。然而，建立表的整体设计将不可避免地导致大量锁冲突时的CPU内核和并行连接在一台服务器上快速增长。因此，锁的粒度重新细化只是优化，而不是彻底解决可扩展性问题。

一个连接中输入输出数据包可以通过两种不同的CPU内核处理，使CPU缓存弹跳性能降解。因此，最大的可扩展性，它是理想的实现完全连接的地方，即对于一个给定的连接的所有活动都在同一个CPU核心处理，包括被动和主动的关系。

VFS抽象，套接字通过VFS和暴露在用户级应用程序一个socket文件描述符（FD）进行抽象。打开和关闭VFS套接字fd的性能对TCP连接的建立和终止的效率有直接影响。然而，在处理VFS共享状态有严重的同步开销，如inode和dentry，导致可扩展性的瓶颈。

在一个8核产品服务器上面运行HAProxy作为HTTP负载平衡器使得微博客户端流量分配到多个服务器的中，这其中会发生严重的锁争用。从收集的性能数据进行分析，我们观察到在TCB和VFS分别占到CPU周期自旋锁消耗总量的9%和11%管理。
更糟糕的是，由于网络堆栈处理内部冲突，在所有的CPU内核有一个明确的负载不平衡，虽然包被网卡均匀地分布到这些CPU内核。

对于全局共享数据结构进行分区，听表建立表；正确引导受到的数据包，实现对任意连接的连接局部性；提供在VFS解决可扩展性问题的快速方法、在VFS上述TCP设计充分保留BSD套接字API。引入一个内核网络协议栈的设计实现了对TCB管理表分区和实现对任意连接类型局部连接。分区方案完整和自然的解决网络处理潜在冲突的问题，包括不能直接解的问题比如定时器锁争用。

在内核中进行适当修改，可以建立一个高度可扩展的内核网络协议栈。通过集成到内核框架，设计可以保持鲁棒性和实现内核网络协议栈的所有功能，这使得它的实际生产环境的部署很实际。fastsocket遵循这一理念，被大量部署在新浪微博的生产环境。

### fast socket设计方案


fastsocket由三部分组成，**TCB数据结构分区**，**接收流传递（RFD）**，和**fastsocket aware VFS**。

三模块合作提供每个核心流程中的所有活动区域，对于一个给定的连接，从网卡中断到访问的应用程序，由一个交叉核同步，单CPU核执行。

>* 使用TCB数据分区的设计思想分流不同的连接:将主动连接和被动连接进行分别区分，在每个核上，建立完全分区的数据结构和管理全局TCB表，实现一条连接的快速查找。
>* 利用RFD技术保证同一条流hash到同一个核上，避免额外的核调度的开销。也消除了共享资源的互访带来的开销。
>* 利用fastsocketa-ware VFS保证兼容性的同时，消除了传统VFS中iNode和dentry访问时候复杂的开销，内部定制接口的提高可扩展性而对外保持VFS兼容Socket应用程序和系统工具。

### 评估和分析
在测试阶段，主要测试了扩展的有效性和对系统的吞吐性能的提升。

>* 在基于HAProxies的sina微博测试环境中，完成同样的数据处理能力，基于fastsocket的系统能有效降低CPU的占用率。
>* 基于Nginx的高并发环境中，基于fastsocket的系统能够处理475K/second的连接请求，传统的TCP只能达到284K/S
>* 在HAProxy测试床中对完成连接的速度对比测试显示，基于fastsocket的连接能力依然比传统的好。

在一个8核的服务器上面测试fastscoket,运行HaProxy作为负载均衡，相比基本的linux性能提升很大。在作者的评估显示，fastsocket在24核机器上面短连接负载下达到加速20.4x，优于目前最先进的Linux内核的TCP实现。当扩展到24个CPU内核时候，fastsocket在Nginx和HAProxy上面的吞吐量与基础Linux内核的比较分别增加267%和621%。证明fastsocket可以同时实现可扩展性和维护BSD套接字API。此外，FastSocket已经部署在sina weibo的产品环境中，服务于5000万日常活跃的用户和每天数十亿的请求。
### 感想

作者在此提出了定制化的VFS，保留了传统的VFS特性，比如防火墙等，这是一种减轻复杂锁的一种办法，但是个人觉得没从根本上解决这些共享资源访问的问题。

与此同时，网络协议栈目前被设计进入Linux的内核中去，但是网络应该独立于操作系统，传统的TCP是由网卡加载到内核的RX_BUF中，然后用户程序在需要网络数据时，再进一步拷贝到用户自己的空间去，据了解，经过内核的拷贝也是造成性能开销的重要原因，同事其中的拷贝经过复杂的系统调用，可能会产生一些锁等，因此如何避免这些额外的拷贝也是一个研究的问题。
